---
title: "Monitoring Model Inputs"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2); library(ggExtra); library(tibble); library(dplyr); library(reshape2)
```

## Problem Statement

We want to monitor model inputs with respect to the empirical "distribution'' of the data used to train a model. The mechanism should alert the system and/or user when unusual-covariates criteria are met. This document gives a structure for this procedure.

## Explanatory Example

Consider a hypothetical dataset of 1000 observations of two covariates, X and Y, generated for this example from two independent $\text{N}(0,1)$ random variables.

```{r, echo=FALSE, fig.align='center'}
set.seed(123)
dat <- data.frame(Category = "Train", X = rnorm(1000), Y = rnorm(1000))
dat <- rbind.data.frame(dat, data.frame(Category = "Flagged", X = 3.5, Y = 3.5))
ggplot(data = dat, aes(x = X, y = Y, color = Category)) + geom_point() + coord_fixed() 

```
The "Flagged" point suggests an irregularity that the user should know about, and may motivate model retraining. The algorithm will cannot rely on a plot, but should instead assess the "distance" of a new covariate from the training covariates, and raise a flag when an outlier is detected.

## Covariate Flag

In the unrealistic but instructive case of covariates with known distributions, the procedure is quite simple. The probability of covariates as extreme or more extreme than that which the model receives is easy to calculate. For example, suppose at some point the engine receives the "Flagged"" covariate from the plot. The probability of a covariate where $X > 3.5$ and $Y > 3.5$, under the assumed conditions, is `r min(pnorm(3.5, lower.tail = TRUE), pnorm(3.5, lower.tail = FALSE))^2`
Such a low probability covariate indicates the conditions may have changed, necessitating a model refit. 

In practice we usually won't know the theoretical distribution of the covariates, but we can calculate measures of empirical likelihood for live data.

## Covariate Empirical "Distributions""
Consider a set of five covariates with unknown distributions.
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
dat <- data.frame(A = round(rchisq(1000, 1), 3), 
                  B = round(rgamma(1000, 1, 1), 3), 
                  C = rbinom(1000, 10, 0.25),
                  X = round(rnorm(1000), 3), 
                  Y = round(runif(1000), 3),
                  Z = round(rexp(1000), 3))
dat2 <- melt(dat)

head(dat)
ggplot(data = dat2, aes(x = value)) + geom_histogram() + facet_wrap(~ variable, scales = "free")
```
Suppose, based on our hypothetical knowledge of the data, and the empirical distributions depicted in the histograms, we decide that a covariate meeting following criteria should be flagged:

* A > 1
* B > 2
* C > 1
* |X| > 1 
* |Y| < 0.5
$ Z > 1

Then we can find those observations meeting the criteria, as a proportion of n, to calcuate an empirical probability of meeting the criteria.

```{r}
nrow(filter(dat,
            A > 1,
            B > 2,
            C > 1,
            abs(X) > 1,
            abs(Y) < 0.5,
            Z > 1)
     ) / nrow(dat)
```
In this case, the empirical probability of meeting the criteria is 0.003. That is, of the observations used to train the (hypothetical model), only 3 of 1,000 met this criteria.

## Generalizing

This approach is easily generalizable _given_ a set of criteria. However, detecting multivariate outliers is a nontrivial problem. Below are a few options.

```{r}

```